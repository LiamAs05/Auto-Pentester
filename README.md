# Automatic Penetration Tester
Created as a final project for "Magshimim - National Cybersecurity Program".
This project concentrates on making the internet a safer place by protecting websites from potential vulnerabilities.

We are able to scan your website for the following vulnerabilities:
* _Cross Site Scripting_
* _Command Injection_ 
* _SQL Injection_
* _Cross Site Request Forgery_

## The Approach
We manage to protect websites using a variety of relevant tools and skills.
* _Python_: Selenium, Requests, BeautifulSoup, SQLModel, Scapy.
* _OOD_: An abstract, easy-to-use API following the OOP paradigms.
* _SQLite_: Database of cherry-picked payloads to trigger vulnerabilities 
* _Backend Web Development_: Flask, Node.js
* _Penetration Testing_
* _Parallel Programming_
* _Docker_

All of this combines into a single `main.py` file that, when run, scans your websites and alerts you about potential security holes.

## The Injection Algorithm 
Used partially or fully in the `XSS` and `Command Injection` scans.   
1. Parse the input json file provided by the user using _JSON_.
2. Scan the website and find all the routes (pages) using _Requests_.
3. Scan each page and parse the elements it contains using _BeautifulSoup_.
4. For all inputs elements found, attempt injecting a special payload using _Selenium_.
5. If the payload has echoed in it's escaped form, this input box is safe. Move on to the next one.
6. If the payload is not escaped using HTML escaping, our program will alert you with a warning.
7. Attempt injecting several other payloads and check the results.
8. Report the final findings to the user.

# The Proof-Of-Defense Algorithm
For ori, specify about CSRF here

## How Do I Start?
To start the project, simply run these commands:
```bash
git clone https://gitlab.com/OriLev1/auto-pentester.git
cd auto-pentester
sudo docker build -t auto-pt:v1 .
sudo docker run --rm -it --name auto-pt auto-pt:v1
```

## The Configuration File

The configuration file is the one and only way to provide output to the program.
It consists of several customizable options to alter the flow of the program to your needs.
Our suggestion is to leave all the fields as default and only change the `url`, but if you do insist,
the fields that you may care about modifying are marked with `IMPORTANT` in the example below.
The `scans` field specifies which scans will be used.
```json
{
    "url": "<website url>",
    "headers": ["optional: provide HTTP headers"],
    "cookies": ["optional: provide HTTP cookies"],
    "auth": ["optional: provide HTTP authentication creds"],
    "use_requests_interface": "specify interface",
    "hidden": "show browser windows",
    "timeout": "set max timeout of crawler",
    "blacklist": ["IMPORTANT: specify pages to avoid"],
    "max_pages": "IMPORTANT: specify max amount of pages to scan",
    "recursive": "IMPORTANT: specify whether or not to recurse crawler to subpages"]
    "plugins": [
        "src.analyzer.webElements.a",
        "src.analyzer.webElements.area",
        "src.analyzer.webElements.base",
        "src.analyzer.webElements.button",
        "src.analyzer.webElements.img",
        "src.analyzer.webElements.link",
        "src.analyzer.webElements.script"
    ],
    "scans": [
        "src.scanManager.scans.xss",
        "src.scanManager.scans.csrf",
        "src.scanManager.scans.sqli",
        "src.scanManager.scans.ci"
    ]
}
```
