#! /usr/bin/python3

from bs4 import BeautifulSoup
import threading

from analyzer.analyzer import Analyzer
from website import Website


# todo Communicator

class Spider:

    def __init__(self, base_url: str, blacklist: list[str], cookies: dict, auth: tuple[str, str]):
        self.base_url = base_url
        self.blacklist = blacklist
        self.cookies = cookies
        self.auth = auth
        self.communicator = None
        self._current_url = self.base_url
        self._website = Website(self.base_url)
        self._parsed_pages = set()
        self._parsed_routes = set()
        self._route_to_parse = set()
        self._pages_lock = threading.Lock()
        self._route_to_parse_lock = threading.Lock()

        # todo add base route to to_parse set
        # todo Register communicator auth and cookies

    def logic(self) -> None:
        Analyzer.parse_page(bytes(""))

    def get_website(self) -> Website:
        return self._website

    @staticmethod
    def _is_link_valid(link):
        # todo find new way to block links
        invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.php', '.aspx', '.xml', '.json']
        skip = ['/']
        start_skip = ['http', 'www']

        for name in skip:
            assert not link == name

        for start in start_skip:
            assert not link.startswith(start)

        for end in invalid_files:
            assert not link.endswith(end)

    @staticmethod
    def _get_all_valid_routes_from_page(plain_page: str) -> set[str]:

        all_valid_routes = set()
        soup = BeautifulSoup(plain_page, 'html.parser')

        elements_dict = {
            'a': 'href',
            'area': 'href',
            'base': 'href',
            'link': 'href',
            'form': 'action'
        }

        for element_type, tag in elements_dict.items():
            for element in soup.find_all(element_type):
                route = element.get(tag)

                if Spider._is_link_valid(route):
                    all_valid_routes.add(route)

        return all_valid_routes


def main():
    url = 'http://quotes.toscrape.com'
    current_url = '/'
    passed_pages = {}
    in_process = set()
    in_process.add(current_url)

    while bool(in_process):

        temp_url = in_process.pop()
        current_url = url + temp_url

        try:
            pages = parse_page(get_page(current_url).text)
        except (AssertionError, requests.RequestException):
            print(f"[*] ParsingError: {current_url}")
            continue

        passed_pages[temp_url] = pages

        in_process = in_process.union(set(pages)) - set(passed_pages.keys())

        print(f'[*] Processing {current_url} successfully')

    print(passed_pages.keys())


if __name__ == '__main__':
    main()
