#! /usr/bin/python3

from bs4 import BeautifulSoup
import threading
from website import Website


class Spider:

    def __init__(self, base_url: str):
        self.base_url = base_url
        self.current_url = self.base_url
        self.website = Website(self.base_url)
        self.parsed_pages = set()
        self.parsed_routes = set()
        self.route_to_parse = set()
        self.pages_lock = threading.Lock()
        self.route_to_parse_lock = threading.Lock()

    def logic(self) -> None:
        pass

    def get_website(self) -> Website:
        return self.website



def get_page(url):
    return res


def valid_link(link):
    invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.php', '.aspx', '.xml', '.json']
    skip = ['/', '/login']
    start_skip = ['http', 'www']

    for name in skip:
        assert not link == name

    for start in start_skip:
        assert not link.startswith(start)

    for end in invalid_files:
        assert not link.endswith(end)


def parse_page(page):

    all_valid_links = set()

    tags_links_dict = {
        'a': 'href',
        'area': 'href',
        'base': 'href',
        'link': 'href',
        'form': 'action'
    }

    soup = BeautifulSoup(page, 'html.parser')

    for link, tag in tags_links_dict.items():
        for element in soup.find_all(link):
            try:
                ln = element.get(tag)
                valid_link(ln)

                all_valid_links.add(ln)
            except (AssertionError, ValueError):
                pass

    return all_valid_links


def main():
    url = 'http://quotes.toscrape.com'
    current_url = '/'
    passed_pages = {}
    in_process = set()
    in_process.add(current_url)

    while bool(in_process):

        temp_url = in_process.pop()
        current_url = url + temp_url

        try:
            pages = parse_page(get_page(current_url).text)
        except (AssertionError, requests.RequestException):
            print(f"[*] ParsingError: {current_url}")
            continue

        passed_pages[temp_url] = pages

        in_process = in_process.union(set(pages)) - set(passed_pages.keys())

        print(f'[*] Processing {current_url} successfully')

    print(passed_pages.keys())


if __name__ == '__main__':
    main()
