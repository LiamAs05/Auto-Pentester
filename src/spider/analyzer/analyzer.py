#! /usr/bin/python3


import threading
from bs4 import BeautifulSoup

from page import Page


class Analyzer:

    @staticmethod
    def parse_page(plain_page: str,
                   route: str,
                   parent_route: str,

                   pages: set[Page],
                   route_to_parse: dict[str, str],
                   blacklist: set[str],

                   pages_lock: threading.Lock,
                   route_to_parse_lock: threading.Lock) -> None:

        page = Page(route)

        # todo put children
        pass
        '''
        passed_pages[temp_url] = pages

        in_process = in_process.union(set(pages)) - set(passed_pages.keys())
        '''

    @staticmethod
    def _is_link_valid(link):
        invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.xml', '.json']
        start_skip = ['http', 'www']

        if link == '/':
            return False

        for start in start_skip:
            if link.startswith(start):
                return False

        for end in invalid_files:
            if link.endswith(end):
                return False

        return True

    @staticmethod
    def _get_all_valid_routes_from_page(plain_text: str, blacklist: set[str]) -> set[str]:

        all_valid_routes = set()
        soup = BeautifulSoup(plain_text, 'html.parser')

        elements_dict = {
            'a': 'href',
            'area': 'href',
            'base': 'href',
            'link': 'href',
            'form': 'action'
        }

        for element_type, tag in elements_dict.items():
            for element in soup.find_all(element_type):
                route = element.get(tag)

                if Analyzer._is_link_valid(route):
                    all_valid_routes.add(route)

        all_valid_routes = all_valid_routes - blacklist

        return all_valid_routes
