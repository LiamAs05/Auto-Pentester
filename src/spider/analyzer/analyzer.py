#! /usr/bin/python3

import threading

from bs4 import BeautifulSoup
from requests import Response

from typing import Type

from src.spider.analyzer.page import Page
from src.spider.analyzer.webElements.IElement import WebElement
from src.spider.analyzer.CostumContextManager import TrialContextManager


from src.spider.analyzer.webElements.form import Form as FormElement
from src.spider.analyzer.webElements.input import Element as InputElement


class Analyzer:

    @staticmethod
    def parse_page(response: Response,
                   base_url: str,
                   route: str,
                   parent_route: str,

                   plugin_elements: list[Type[WebElement]],

                   pages: set[Page],
                   route_to_parse: dict[str, str],
                   blacklist: set[str],

                   pages_lock: threading.Lock,
                   route_to_parse_lock: threading.Lock,

                   auth,
                   is_session,
                   need_auth) -> Page:

        plain_page = response.text

        trial_context_manager = TrialContextManager()
        soup = BeautifulSoup(plain_page, 'html.parser')

        # add route to parse
        all_valid_links = Analyzer._get_all_valid_routes_from_page(plain_page, base_url, blacklist)

        with route_to_parse_lock:

            for valid_link in all_valid_links:
                # link: route (new route: parent route)
                route_to_parse[valid_link] = route

        # create a new page object
        page = Page(
            route=route,
            parent=parent_route,
            children=all_valid_links,
            auth=auth,
            cookies=response.cookies.get_dict(),
            is_session=is_session,
            need_auth=need_auth
        )

        # put title, content_type

        with trial_context_manager:
            page.content_type = soup.find('meta')['content']

        with trial_context_manager:
            page.title = soup.find('title').string

        with trial_context_manager:
            page.forms = Analyzer._get_forms(soup)

        for element in plugin_elements:
            elements = Analyzer._get_element_set(soup, element)
            page = Analyzer._register_element_to_page(page, element.get_element_name(), elements)

        with pages_lock:
            pages.add(page)

        return page

    @staticmethod
    def _is_link_valid(link):
        invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.xml', '.json']
        start_skip = ['http', 'www']

        if link == '/':
            return False

        for start in start_skip:
            if link.startswith(start):
                return False

        for end in invalid_files:
            if link.endswith(end):
                return False

        return True

    @staticmethod
    def _get_forms(soup: BeautifulSoup) -> set[FormElement]:
        form_ret = set()

        for form in soup.find_all('form'):
            form_element = FormElement()
            form_attributes = FormElement.get_element_attributes()
            for attribute in form_attributes.keys():
                form_attributes[attribute] = form.attrs.get(attribute, form_attributes[attribute])

            form_element.attributes = form_attributes

            for form_input_element in form.find_all('input'):
                input_element = InputElement()
                input_attributes = input_element.get_element_attributes()

                for attribute in input_attributes:
                    input_attributes[attribute] = form_input_element.attrs.get(attribute, input_attributes[attribute])

                form_element.add_input(input_element)

            form_ret.add(form_element)

        return form_ret

    @staticmethod
    def _get_element_set(soup: BeautifulSoup, abstract_element: Type[WebElement]) -> set[WebElement]:
        all_elements = set()

        for web_element in soup.find_all(abstract_element.get_element_name()):

            elem_cpy = abstract_element()
            element_attributes = elem_cpy.get_element_attributes()

            for attribute in element_attributes:
                element_attributes[attribute] = web_element.attrs.get(attribute, element_attributes[attribute])

            elem_cpy.parse_element(element_attributes)
            all_elements.add(elem_cpy)

        return all_elements

    @staticmethod
    def _register_element_to_page(page: Page, element_name: str, elements: set[WebElement]) -> Page:

        match element_name:
            case 'a':
                page.a_tags = elements

            case 'area':
                page.areas = elements

            case 'base':
                page.bases = elements

            case 'link':
                page.links = elements

            case 'img':
                page.images = elements

            case 'script':
                page.scripts = elements

            case _:
                page.add_element(elements)

        return page

    @staticmethod
    def _get_all_valid_routes_from_page(plain_text: str, base_url, blacklist: set[str]) -> set[str]:

        all_valid_routes = set()
        soup = BeautifulSoup(plain_text, 'html.parser')

        elements_dict = {
            'a': 'href',
            'area': 'href',
            'base': 'href',
            'link': 'href',
            'form': 'action'
        }

        for element_type, tag in elements_dict.items():
            for element in soup.find_all(element_type):
                route = element.get(tag)

                if route.startswith(base_url):
                    route = route[len(base_url):]
                    if not route.startwith('/'):
                        route = '/' + route

                if Analyzer._is_link_valid(route):
                    all_valid_routes.add(route)

        all_valid_routes = all_valid_routes - blacklist

        return all_valid_routes
