#! /usr/bin/python3

import threading

from bs4 import BeautifulSoup
from requests import Response

from typing import Type

from src.analyzer.page import Page
from src.analyzer.webElements.IElement import WebElement
from src.analyzer.CostumContextManager import TrialContextManager


from src.analyzer.webElements.form import Form as FormElement
from src.analyzer.webElements.input import Element as InputElement


class Analyzer:

    @staticmethod
    def parse_page(response: Response,
                   base_url: str,
                   route: str,
                   parent_route: str,

                   plugin_elements: list[Type[WebElement]],

                   pages: set[Page],
                   route_to_parse: dict[str, str],
                   blacklist: set[str],

                   pages_lock: threading.Lock,
                   route_to_parse_lock: threading.Lock,

                   auth,
                   is_session,
                   need_auth) -> Page:
        """
        Function get a response to http request, analyze it and put it in a Page costume object
        @param response: a http response object
        @param base_url: the base url of the website
        @param route: toute of the current request
        @param parent_route: parent route of the current request
        @param plugin_elements: a list of plugin elements (WebElement)
        @param pages: a list of all pages, add thr new page to this list
        @param route_to_parse: route to parse, analyze and append new route to list
        @param blacklist: a blacklist of routes
        @param pages_lock: lock to pages object
        @param route_to_parse_lock: lock to route to parse object
        @param auth: authentication data
        @param is_session: is the current route is session
        @param need_auth: is the current route need authentication
        @return: a page object
        """

        plain_page = response.text

        trial_context_manager = TrialContextManager()
        soup = BeautifulSoup(plain_page, 'html.parser')

        page = Page(
            route=route,
            parent=parent_route,
            auth=auth,
            cookies=response.cookies.get_dict(),
            is_session=is_session,
            need_auth=need_auth
        )

        # add route to parse
        all_valid_links = Analyzer._get_all_valid_routes_from_page(plain_page, base_url, blacklist)

        with route_to_parse_lock:

            for valid_link in all_valid_links:
                # link: route (new route: parent route)
                route_to_parse[valid_link] = route

        page.children = all_valid_links

        with trial_context_manager:
            page.content_type = soup.find('meta')['content']

        with trial_context_manager:
            page.title = soup.find('title').string

        with trial_context_manager:
            page.forms = Analyzer._get_forms(soup)

        for element in plugin_elements:
            elements = Analyzer._get_element_set(soup, element)
            page = Analyzer._register_element_to_page(page, element.get_element_name(), elements)

        with pages_lock:
            pages.add(page)

        return page

    @staticmethod
    def _is_link_valid(link: str) -> bool:
        """
        Function check if link is valid (valid means link in the base url, and not a file (img/json/...)

        @param link: a link to check
        @return: if the link is valid
        """
        invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.xml', '.json']
        start_skip = ['http', 'www']

        if link in ('/', '', '#'):
            return False

        for start in start_skip:
            if link.startswith(start):
                return False

        for end in invalid_files:
            if link.split('?')[0].endswith(end):
                return False

        return True

    @staticmethod
    def _get_forms(soup: BeautifulSoup) -> set[FormElement]:
        """
        FUnction find all the <form> tags in the document

        @param soup: a BeautifulSoup object
        @return: a set of FormElement
        """
        form_ret = set()

        for form in soup.find_all('form'):
            form_element = FormElement()
            form_attributes = FormElement.get_element_attributes()
            for attribute in form_attributes.keys():
                form_attributes[attribute] = form.attrs.get(attribute, form_attributes[attribute])

            form_element.attributes = form_attributes

            # scrape the <input> 's of the form element
            for form_input_element in form.find_all('input'):
                input_element = InputElement()
                input_attributes = input_element.get_element_attributes()

                for attribute in input_attributes.keys():
                    input_attributes[attribute] = form_input_element.attrs.get(attribute, input_attributes[attribute])

                input_element.attributes = input_attributes
                form_element.add_input(input_element)

            form_ret.add(form_element)

        return form_ret

    @staticmethod
    def _get_element_set(soup: BeautifulSoup, abstract_element: Type[WebElement]) -> set[WebElement]:
        """
        Function find in the document elements and map the

        @param soup: a BeautifulSoup object
        @param abstract_element: Element (from plugins) to scrape
        @return: set of  elements
        """
        all_elements = set()

        for web_element in soup.find_all(abstract_element.get_element_name()):

            elem_cpy = abstract_element()
            element_attributes = elem_cpy.get_element_attributes()

            for attribute in element_attributes.keys():
                element_attributes[attribute] = web_element.attrs.get(attribute, element_attributes[attribute])

            elem_cpy.parse_element(element_attributes)
            all_elements.add(elem_cpy)

        return all_elements

    @staticmethod
    def _register_element_to_page(page: Page, element_name: str, elements: set[WebElement]) -> Page:
        """
        Function register the element to the page
        @param page: Page object
        @param element_name: name of the element
        @param elements: set of the elements
        @return: new Page object
        """

        match element_name:
            case 'a':
                page.a_tags = elements

            case 'area':
                page.areas = elements

            case 'base':
                page.bases = elements

            case 'link':
                page.links = elements

            case 'img':
                page.images = elements

            case 'script':
                page.scripts = elements

            case _:
                page.add_element(elements)

        return page

    @staticmethod
    def _get_all_valid_routes_from_page(plain_text: str, base_url, blacklist: set[str]) -> set[str]:
        """
        Function find all the sub-links in the document
        @param plain_text: the plain document text
        @param base_url: the base URL of the website
        @param blacklist: a blacklist (links cannot be in the blacklist)
        @return: a set of links
        """

        all_valid_routes = set()
        soup = BeautifulSoup(plain_text, 'html.parser')

        elements_dict = {
            'a': 'href',
            'area': 'href',
            'base': 'href',
            'link': 'href',
            'form': 'action'
        }

        for element_type, tag in elements_dict.items():
            for element in soup.find_all(element_type):
                route = element.get(tag)

                if route.startswith(base_url):
                    route = route[len(base_url):]
                    if not route.startwith('/'):
                        route = '/' + route

                if Analyzer._is_link_valid(route):
                    all_valid_routes.add(route)

        all_valid_routes = all_valid_routes - blacklist

        return all_valid_routes
