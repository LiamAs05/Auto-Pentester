#! /usr/bin/python3

import threading

from bs4 import BeautifulSoup
from bs4 import Comment
from requests import Response

from typing import Type

from src.analyzer.page import Page
from src.analyzer.webElements.IElement import WebElement
from src.analyzer.CustomContextManager import TrialContextManager

from src.classificator.classificator import Classificator


class Analyzer:

    @staticmethod
    def parse_page(response: Response,
                   base_url: str,
                   route: str,
                   parent_route: str,

                   plugin_elements: list[Type[WebElement]],

                   pages: set[Page],
                   route_to_parse: dict[str, str],
                   blacklist: set[str],

                   pages_lock: threading.Lock,
                   route_to_parse_lock: threading.Lock,

                   auth: tuple,
                   is_session: bool,
                   need_auth: bool) -> None:
        """
        Function get a response to http request, analyze it and put it in a Page costume object
        @param response: a http response object
        @param base_url: the base url of the website
        @param route: toute of the current request
        @param parent_route: parent route of the current request
        @param plugin_elements: a list of plugin elements (WebElement)
        @param pages: a list of all pages, add thr new page to this list
        @param route_to_parse: route to parse, analyze and append new route to list
        @param blacklist: a blacklist of routes
        @param pages_lock: lock to pages object
        @param route_to_parse_lock: lock to route to parse object
        @param auth: authentication data
        @param is_session: is the current route is session
        @param need_auth: is the current route need authentication
        @return: None
        """

        plain_page = response.text

        trial_context_manager = TrialContextManager()
        soup = BeautifulSoup(plain_page, 'html.parser')

        page = Page(
            response=response,
            route=route if route else "/",
            parent=parent_route,
            auth=auth,
            cookies=response.cookies.get_dict(),
            is_session=is_session,
            need_auth=need_auth
        )
        # add route to parse
        all_valid_links = Analyzer._get_all_valid_routes_from_page(plain_page, base_url, blacklist)

        route_to_parse_lock.acquire()

        for valid_link in all_valid_links:
            # link: route (new route: parent route)
            route_to_parse[valid_link] = route

        route_to_parse_lock.release()

        page.children = all_valid_links

        with trial_context_manager:
            page.content_type = soup.find('meta')['content']

        with trial_context_manager:
            page.title = soup.find('title').string

        with trial_context_manager:
            page.forms = Classificator.get_forms(soup)

        for element in plugin_elements:
            elements = Analyzer._get_element_set(soup, element)
            page = Analyzer._register_element_to_page(page, element.get_element_name(), elements)

        page.type = Classificator.classification(page)

        pages_lock.acquire()
        pages.add(page)
        pages_lock.release()

    @staticmethod
    def _is_link_valid(link: str) -> bool:
        """
        Function check if link is valid (valid means link in the base url, and not a file (img/json/...)

        @param link: a link to check
        @return: if the link is valid
        """
        invalid_files = ['.js', '.ts', '.css', '.png', '.jpg', '.jpeg', '.gif', '.xml', '.json']
        start_skip = ['http', 'www']

        if link in ('/', '', '#'):
            return False

        for start in start_skip:
            if link and link.startswith(start):
                return False

        for end in invalid_files:
            if link and link.split('?')[0].endswith(end):
                return False

        return True

    @staticmethod
    def _get_element_set(soup: BeautifulSoup, abstract_element: Type[WebElement]) -> set[WebElement]:
        """
        Function find in the document elements and map the

        @param soup: a BeautifulSoup object
        @param abstract_element: Element (from plugins) to scrape
        @return: set of  elements
        """
        all_elements = set()

        for web_element in soup.find_all(abstract_element.get_element_name()):

            elem_cpy = abstract_element()
            element_attributes = elem_cpy.get_element_attributes()

            for attribute in element_attributes.keys():
                element_attributes[attribute] = web_element.attrs.get(attribute, element_attributes[attribute])

            elem_cpy.parse_element(element_attributes)
            all_elements.add(elem_cpy)

        return all_elements

    @staticmethod
    def _register_element_to_page(page: Page, element_name: str, elements: set[WebElement]) -> Page:
        """
        Function register the element to the page class
        @param page: Page object
        @param element_name: name of the element
        @param elements: set of the elements
        @return: new Page object
        """

        match element_name:
            case 'a':
                page.a_tags = elements

            case 'area':
                page.areas = elements

            case 'base':
                page.bases = elements

            case 'link':
                page.links = elements

            case 'img':
                page.images = elements

            case 'script':
                page.scripts = elements

            case _:
                page.add_element(elements)

        return page

    @staticmethod
    def _get_all_valid_routes_from_page(plain_text: str, base_url, blacklist: set[str]) -> set[str]:
        """
        Function find all the sub-links in the document
        @param plain_text: the plain document text
        @param base_url: the base URL of the website
        @param blacklist: a blacklist (links cannot be in the blacklist)
        @return: a set of links
        """

        all_valid_routes = set()
        soup = BeautifulSoup(plain_text, 'html.parser')

        elements_dict = {
            'a': 'href',
            'area': 'href',
            'base': 'href',
            'link': 'href',
            'form': 'action'
        }

        for element_type, tag in elements_dict.items():
            for element in soup.find_all(element_type):
                route = element.get(tag)

                if route and route.startswith(base_url):
                    route = route[len(base_url):]
                    if not route.startswith('/'):
                        route = '/' + route

                if Analyzer._is_link_valid(route):
                    all_valid_routes.add(route)

        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            for sub_comment in str(comment):
                if sub_comment.startswith('/'):
                    all_valid_routes.add(sub_comment)

        all_valid_routes = all_valid_routes - blacklist

        return all_valid_routes
